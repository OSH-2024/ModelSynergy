# 可行性报告

## 摘要
---

## 项目背景
---

### 操作系统集成LLM

未来操作系统的交互性与大型语言模型（LLM）的结合将开启新的交互模式，并极大地扩展操作系统的智能化水平。与现有的操作系统交互方式相比，集成了LLM的新型交互方式将大大提升用户友好性，帮助用户更高效、便捷地完成复杂任务。

#### 交互能力
**集成了LLM的操作系统将能够提供更自然语言的交互能力**。用户可以通过自然语言来命令和控制操作系统及其应用，类似于与真人进行交流。这种交互方式降低了学习使用计算机的门槛，使得技术更加亲民和便捷。

#### 处理能力
**LLM可以增强操作系统的上下文理解和处理能力**。结合了LLM后，系统能更好地理解用户的指令和查询意图，即使是复杂的多步骤任务或长篇文档分析也能得到更有效的处理。

#### 决策支持
**操作系统可以利用LLM进行决策支持和问题解决**。例如，AI2团队提出的SwiftSage框架结合了模仿学习和LLM方法的优势，展现出解决复杂数字任务的巨大潜力。这表明未来的操作系统不仅仅是执行指令的工具，而是可以深度参与到问题解决过程中，成为用户的助手。

#### 安全性
**操作系统的安全性和稳定性仍将由其内核维护**。尽管LLM强大，但它通过系统调用接口与操作系统交互，确保了系统的完整性和效率。这意味着LLM并不直接管理硬件资源，而是在操作系统的管理下运行，保持了必要的抽象层和安全距离。

#### LLM危险操作时的系统保护

**确保操作系统在LLM执行危险操作时的保护是非常必要的**。


- **输出不可预测性**：尽管LLM通常能够产生合理的回答，但在某些情况下，它们可能会产生不准确或不合适的内容，这可能会导致不良后果。
- **工具链安全问题**：在LLM服务的开发和部署生命周期中，涉及的工具链可能存在安全漏洞，这些漏洞可能被利用来攻击LLM系统。

为了应对这些风险，需要采取一系列安全措施来保护操作系统：

- **输入验证**：对用户输入进行严格的验证，以防止恶意代码注入或引导LLM产生不当输出。
- **输出监控**：实时监控LLM的输出，以便及时发现并处理不准确或不适当的内容。
- **权限控制**：限制LLM的访问权限，确保只有经过授权的用户才能与模型交互。
- **系统备份**：在LLM进行危险操作，例如删除文件等，进行系统快照备份，便于用户回滚。

综上所述，由于LLM可能引发的安全威胁，以及潜在的风险和后果，确保操作系统具备充分的保护措施是非常重要的。

#### LLM上下文长度外推

在应用大语言模型时，上下文长度往往是提升模型能力的瓶颈。如在使用检索增强技术（RAG）时，模型会被要求利用若干篇文档，而这可能超出模型的上下文长度限制。对于一般的模型来说，超出上下文长度之后，困惑度会急剧上升，极大地损害了模型的性能。早期的大语言模型的上下文长度限制很小，为4K tokens, 最新的模型可以达到1M tokens，然而仍然存在着上下文长度不足的问题。

在LLM结合操作系统过程中，上下文长度是尤其重要的限制。在某些情况下，或许整个文件系统都需要成为LLM的上下文。另外，如果需要构建AI本地知识库，更需要较长上下文的灵活选择与切换。

​大语言模型的上下文长度限制来自于两方面：从训练来看，由于transformer的平方复杂度，提升训练时的文本长度代价很高；从推理来看，同样由于transformer的平方复杂度，推理的代价很大。

​这催生了许多关于上下文长度外推的研究。上下文长度外推是指将在较短文本（如4k）上训练的大语言模型应用到更长的上下文上，同时降低模型的困惑度。一些研究同时也能降低推理时的资源占用。这些研究包括InfLLM，CEPE，infinityLLM等。

### 现有AI操作系统理论及成果

#### AIOS
**为了充分发挥LLM的潜能，可能需要设计新型的操作系统架构**。例如Rutgers大学提出的AIOS——一种LLM智体操作系统，将大语言模型嵌入到操作系统中作为“大脑”，实现了具有高度智能的操作系统。这样的系统可能具备自我学习和适应的能力，能够根据用户行为和偏好自动优化体验。

#### Personal LLM Agent
**Agent并不直接参与到内核的管理和调度中，而是而是在内核层与用户层之间作为桥梁**。设想中的Agent在内核层面能够与操作系统紧密耦合，高效地实现系统调用等任务；在应用程序层面能学习各种应用程序的使用，并在执行实际任务时自主决策、调用工具；在与用户交互层面能理解用自然语言描述的复杂指令，学习用户的使用习惯，自主调度文件系统以适应用户需求。

#### AI PC
![AIPC](./image/AI%20PC.jpg)
**AI PC是许多软硬件厂商聚焦的热点产品，也是目前完成度最高的“AI for OS”方案**。3月26日，英特尔联合许多合作伙伴出席“北京2024全新英特尔商用客户端AI PC产品发布会”，着重介绍了新一代AI PC产品的特色功能及行业的未来发展。上图介绍了AI PC的六大关键应用，其中尤其值得关注的是AIPC助理和AI本地知识库。在发布会现场的演示中，AIPC助理能完成一系列任务，包括根据较模糊的描述查找特定文件；AI本地知识库可以通过RAG技术对文档进行切分、存储、本地化检索，并提供准确的数据分析。

## 理论依据
---

### langchain

### InfLLM
为了让大模型能够记忆并处理更长的上下文，来自清华大学、麻省理工学院和人民大学的研究人员联合提出无需额外训练的大模型长文本理解方法`InfLLM`，利用少量计算和显存开销实现了LLM的超长文本处理。

`InfLLM`是一种无需训练的记忆增强方法，用于流式地处理超长序列，提升大模型的长度泛化能力。考虑到长文本注意力的稀疏性，处理每个Token通常只需要其上下文的一小部分。研究者构建了一个外部记忆模块，用于存储超长上下文信息；采用滑动窗口机制，每个计算步骤，只有与当前Token距离相近的Tokens（Local Tokens）和外部记忆模块中的少量相关信息参与到注意力层的计算中，而忽略其他不相关的噪声。因此，LLMs可以使用有限的窗口大小来理解整个长序列，并避免引入噪声。

​然而，超长序列中的海量上下文对于记忆模块中有效的相关信息定位和记忆查找效率带来了重大挑战。为了应对这些挑战，上下文记忆模块中每个记忆单元由一个语义块构成，一个语义块由连续的若干Token构成。具体而言：

 1. 为了有效定位相关记忆单元，每个语义块的连贯语义比碎片化的Token更能有效满足相关信息查询的需求。此外，研究者从每个语义块中选择语义上最重要的Token，即接收到注意力分数最高的Token，作为语义块的表示，这种方法有助于避免在相关性计算中不重要Token的干扰。
 2. 为了高效的内存查找，语义块级别的记忆单元避免了逐Token、逐注意力的相关性计算，降低了计算复杂性。此外，语义块级别的记忆单元确保了连续的内存访问，并减少了内存加载成本。得益于此，研究者设计了一种针对上下文记忆模块的高效卸载机制（Offloading）。考虑到大多数记忆单元的使用频率不高，`InfLLM`将所有记忆单元卸载到CPU内存上，并动态保留频繁使用的记忆单元放在GPU显存中，从而显著减少了显存使用量。

## 技术依据
---

### 大模型的选择与部署
大模型是我们项目的基础。大模型的选择主要有两个方向：用API接入外部大模型，如Chat-GPT；本地部署较小的开源模型，如llama2。两种选择各有优劣。API接入外部模型的优点主要在于Chat-GPT等模型的自然语言处理和代码生成能力明显较强，缺点是受到网络、成本、VPN等因素的限制。而本地部署开源小模型的优劣与之相反，牺牲了一部分能力换取便捷性、可移植性、可更改性。

考虑到后续的项目实践中可能需要对大模型进行微调，我们目前尝试并初步完成了本地开源小模型的部署。使用ollama平台作为大模型下载、更新、自然语言交互的媒介。借助ollama，我们在不同设备、不同终端上部署并简单测试了llama2、mistral等多个开源小模型。下图是本地运行llama2模型并进行自然语言交流的例子。

![llama2](./image/llama2.png)
~~看起来它非常重视安全与隐私~~

测试后我们也发现了一些问题。首先是最关心的硬件资源问题。对于仅拥有Intel普通核显的笔记本，一般的运行速度是可以接受的，大概与网上使用讯飞星火等大模型的生成速度相当。监控GPU使用率，发现在生成某些复杂回答时，GPU的负荷较高（80%-90%），甚至有可能达到100%阻止回答继续生成。而对于有RTX4050等显卡的笔记本，回答速度可以达到非常快，GPU的负荷也在正常范围内，初步判断可以承担后续项目的任务。其次我们尝试了多个开源小模型，发现目前ollama支持的开源模型的中文能力都比较差，这限制了后续的测试与评估需要使用英语。另外，当我们在Windows系统上尝试一些其他调用本地大模型的项目（比如open-interpreter）时，部署项目总会出现各种各样的问题，使用Linux系统则比较成功。这可能是因为开源项目开发者们都倾向于使用Linux系统。

### 利用`langchain`部署工具链

### 将`InfLLM`与操作系统耦合
在操作系统中应用大语言模型时，往往也需要较长的上下文。举个例子，用户需要大语言模型从若干本地的文档中检索出某些内容，或需要大语言模型理解一整个代码项目以提供辅助。在这种情况下，我们准备将InfLLM与操作系统耦合。

​具体来说，我们准备在本地记录每个文件的kvcache, 这些kvcache会在文件被修改后，在计算资源空闲时调用大语言模型来生成，并在使用大语言模型时被调用。这样可以提高大语言模型的生成速度，同时拓展上下文长度，利用更多的信息。

## 实验与评估方法
---

## 结论与创新
---

## 参考文献
---